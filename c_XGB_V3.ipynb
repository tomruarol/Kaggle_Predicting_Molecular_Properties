{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Script to test the custom modeling .py scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# imports \n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "import gc\n",
    "from numba import jit\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler,Imputer,LabelEncoder,PolynomialFeatures\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "#from catboost import CatBoostRegressor, CatBoostClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "# Model selection\n",
    "from sklearn.model_selection import train_test_split,cross_validate\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_mean_log_mae(y_true, y_pred, types, floor=1e-9):\n",
    "    \"\"\"\n",
    "    Fast metric computation for this competition: https://www.kaggle.com/c/champs-scalar-coupling\n",
    "    Code is from this kernel: https://www.kaggle.com/uberkinder/efficient-metric\n",
    "    \"\"\"\n",
    "    maes = (y_true-y_pred).abs().groupby(types).mean()\n",
    "    return np.log(maes.map(lambda x: max(x, floor))).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling Custom Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_regression(X, X_test, y, params, folds, model_type='lgb', eval_metric='mae', columns=None, plot_feature_importance=False, model=None,\n",
    "                               verbose=10000, early_stopping_rounds=200, n_estimators=50000):\n",
    "    \"\"\"\n",
    "    A function to train a variety of regression models.\n",
    "    Returns dictionary with oof predictions, test predictions, scores and, if necessary, feature importances.\n",
    "    \n",
    "    :params: X - training data, can be pd.DataFrame or np.ndarray (after normalizing)\n",
    "    :params: X_test - test data, can be pd.DataFrame or np.ndarray (after normalizing)\n",
    "    :params: y - target\n",
    "    :params: folds - folds to split data\n",
    "    :params: model_type - type of model to use\n",
    "    :params: eval_metric - metric to use\n",
    "    :params: columns - columns to use. If None - use all columns\n",
    "    :params: plot_feature_importance - whether to plot feature importance of LGB\n",
    "    :params: model - sklearn model, works only for \"sklearn\" model type\n",
    "    \n",
    "    \"\"\"\n",
    "    columns = X.columns if columns is None else columns\n",
    "    X_test = X_test[columns]\n",
    "    \n",
    "    # to set up scoring parameters\n",
    "    metrics_dict = {'mae': {'lgb_metric_name': 'mae',\n",
    "                        'catboost_metric_name': 'MAE',\n",
    "                        'sklearn_scoring_function': metrics.mean_absolute_error},\n",
    "                    'group_mae': {'lgb_metric_name': 'mae',\n",
    "                        'catboost_metric_name': 'MAE',\n",
    "                        'scoring_function': group_mean_log_mae},\n",
    "                    'mse': {'lgb_metric_name': 'mse',\n",
    "                        'catboost_metric_name': 'MSE',\n",
    "                        'sklearn_scoring_function': metrics.mean_squared_error}\n",
    "                    }\n",
    "\n",
    "    \n",
    "    result_dict = {}\n",
    "    \n",
    "    # out-of-fold predictions on train data\n",
    "    oof = np.zeros(len(X))\n",
    "    \n",
    "    # averaged predictions on train data\n",
    "    prediction = np.zeros(len(X_test))\n",
    "    \n",
    "    # list of scores on folds\n",
    "    scores = []\n",
    "    feature_importance = pd.DataFrame()\n",
    "    \n",
    "    # split and train on folds\n",
    "    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n",
    "        print(f'Fold {fold_n + 1} started at {time.ctime()}')\n",
    "        if type(X) == np.ndarray:\n",
    "            X_train, X_valid = X[columns][train_index], X[columns][valid_index]\n",
    "            y_train, y_valid = y[train_index], y[valid_index]\n",
    "        else:\n",
    "            X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n",
    "            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "            \n",
    "        if model_type == 'lgb':\n",
    "            model = lgb.LGBMRegressor(**params, n_estimators = n_estimators, n_jobs = -1)\n",
    "            model.fit(X_train, y_train, \n",
    "                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric=metrics_dict[eval_metric]['lgb_metric_name'],\n",
    "                    verbose=verbose, early_stopping_rounds=early_stopping_rounds)\n",
    "            \n",
    "            y_pred_valid = model.predict(X_valid)\n",
    "            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n",
    "            \n",
    "        if model_type == 'xgb':\n",
    "            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X.columns)\n",
    "            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X.columns)\n",
    "\n",
    "            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n",
    "            model = xgb.train(dtrain=train_data, num_boost_round=400, evals=watchlist, early_stopping_rounds=200, verbose_eval=verbose, params=params)\n",
    "            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n",
    "            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n",
    "        \n",
    "        if model_type == 'sklearn':\n",
    "            model = model\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            y_pred_valid = model.predict(X_valid).reshape(-1,)\n",
    "            score = metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid)\n",
    "            print(f'Fold {fold_n}. {eval_metric}: {score:.4f}.')\n",
    "            print('')\n",
    "            \n",
    "            y_pred = model.predict(X_test).reshape(-1,)\n",
    "        \n",
    "        if model_type == 'cat':\n",
    "            model = CatBoostRegressor(iterations=20000,  eval_metric=metrics_dict[eval_metric]['catboost_metric_name'], **params,\n",
    "                                      loss_function=metrics_dict[eval_metric]['catboost_metric_name'])\n",
    "            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n",
    "\n",
    "            y_pred_valid = model.predict(X_valid)\n",
    "            y_pred = model.predict(X_test)\n",
    "        \n",
    "        oof[valid_index] = y_pred_valid.reshape(-1,)\n",
    "        if eval_metric != 'group_mae':\n",
    "            scores.append(metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid))\n",
    "        else:\n",
    "            scores.append(metrics_dict[eval_metric]['scoring_function'](y_valid, y_pred_valid, X_valid['type']))\n",
    "\n",
    "        prediction += y_pred    \n",
    "        \n",
    "        if model_type == 'lgb' and plot_feature_importance:\n",
    "            # feature importance\n",
    "            fold_importance = pd.DataFrame()\n",
    "            fold_importance[\"feature\"] = columns\n",
    "            fold_importance[\"importance\"] = model.feature_importances_\n",
    "            fold_importance[\"fold\"] = fold_n + 1\n",
    "            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n",
    "\n",
    "    prediction /= folds.n_splits\n",
    "    \n",
    "    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n",
    "    \n",
    "    result_dict['oof'] = oof\n",
    "    result_dict['prediction'] = prediction\n",
    "    result_dict['scores'] = scores\n",
    "    \n",
    "    if model_type == 'lgb':\n",
    "        if plot_feature_importance:\n",
    "            feature_importance[\"importance\"] /= folds.n_splits\n",
    "            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n",
    "                by=\"importance\", ascending=False)[:50].index\n",
    "\n",
    "            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n",
    "\n",
    "            plt.figure(figsize=(16, 12));\n",
    "            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n",
    "            plt.title('LGB Features (avg over folds)');\n",
    "            \n",
    "            result_dict['feature_importance'] = feature_importance\n",
    "        \n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Data\n",
    "train = pd.read_csv('input/train.csv')\n",
    "\n",
    "# Test Data\n",
    "test = pd.read_csv('input/test.csv')\n",
    "\n",
    "# Submission Data\n",
    "sub = pd.read_csv('input/sample_submission.csv')\n",
    "\n",
    "#General Data\n",
    "structures = pd.read_csv('input/structures.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4658147 entries, 0 to 4658146\n",
      "Data columns (total 6 columns):\n",
      "id                          int64\n",
      "molecule_name               object\n",
      "atom_index_0                int64\n",
      "atom_index_1                int64\n",
      "type                        object\n",
      "scalar_coupling_constant    float64\n",
      "dtypes: float64(1), int64(3), object(2)\n",
      "memory usage: 213.2+ MB\n",
      "#################################################\n",
      "#################################################\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2505542 entries, 0 to 2505541\n",
      "Data columns (total 5 columns):\n",
      "id               int64\n",
      "molecule_name    object\n",
      "atom_index_0     int64\n",
      "atom_index_1     int64\n",
      "type             object\n",
      "dtypes: int64(3), object(2)\n",
      "memory usage: 95.6+ MB\n"
     ]
    }
   ],
   "source": [
    "# We take a first look at the dataset\n",
    "train.info()\n",
    "print ('#################################################')\n",
    "print ('#################################################')\n",
    "test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Chemical Bond Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'H': 0.43, 'C': 0.8200000000000001, 'N': 0.8, 'O': 0.78, 'F': 0.76}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33527f7e293e496f883a60d91bf91bbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2358657), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dec905ad26994f0fbdf05c04f326c52e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2358657), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_name</th>\n",
       "      <th>atom_index</th>\n",
       "      <th>atom</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>EN</th>\n",
       "      <th>rad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "      <td>-0.012698</td>\n",
       "      <td>1.085804</td>\n",
       "      <td>0.008001</td>\n",
       "      <td>2.55</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>1</td>\n",
       "      <td>H</td>\n",
       "      <td>0.002150</td>\n",
       "      <td>-0.006031</td>\n",
       "      <td>0.001976</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>2</td>\n",
       "      <td>H</td>\n",
       "      <td>1.011731</td>\n",
       "      <td>1.463751</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>3</td>\n",
       "      <td>H</td>\n",
       "      <td>-0.540815</td>\n",
       "      <td>1.447527</td>\n",
       "      <td>-0.876644</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>4</td>\n",
       "      <td>H</td>\n",
       "      <td>-0.523814</td>\n",
       "      <td>1.437933</td>\n",
       "      <td>0.906397</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      molecule_name  atom_index atom         x         y         z    EN   rad\n",
       "0  dsgdb9nsd_000001           0    C -0.012698  1.085804  0.008001  2.55  0.82\n",
       "1  dsgdb9nsd_000001           1    H  0.002150 -0.006031  0.001976  2.20  0.43\n",
       "2  dsgdb9nsd_000001           2    H  1.011731  1.463751  0.000277  2.20  0.43\n",
       "3  dsgdb9nsd_000001           3    H -0.540815  1.447527 -0.876644  2.20  0.43\n",
       "4  dsgdb9nsd_000001           4    H -0.523814  1.437933  0.906397  2.20  0.43"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "atomic_radius = {'H':0.38, 'C':0.77, 'N':0.75, 'O':0.73, 'F':0.71} # Without fudge factor\n",
    "\n",
    "fudge_factor = 0.05\n",
    "atomic_radius = {k:v + fudge_factor for k,v in atomic_radius.items()}\n",
    "print(atomic_radius)\n",
    "\n",
    "electronegativity = {'H':2.2, 'C':2.55, 'N':3.04, 'O':3.44, 'F':3.98}\n",
    "\n",
    "#structures = pd.read_csv(structures, dtype={'atom_index':np.int8})\n",
    "\n",
    "atoms = structures['atom'].values\n",
    "atoms_en = [electronegativity[x] for x in tqdm(atoms)]\n",
    "atoms_rad = [atomic_radius[x] for x in tqdm(atoms)]\n",
    "\n",
    "structures['EN'] = atoms_en\n",
    "structures['rad'] = atoms_rad\n",
    "\n",
    "display(structures.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating bonds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5771e5e64574dd3bca81afde8cb21c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=27), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Counting and condensing bonds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5141f1197ffb4272a18b224f83f1852e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2358657), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f9c2324c6a7472b873b531c800c6616",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2358657), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_name</th>\n",
       "      <th>atom_index</th>\n",
       "      <th>atom</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>EN</th>\n",
       "      <th>rad</th>\n",
       "      <th>bond_lengths_mean</th>\n",
       "      <th>bond_lengths_std</th>\n",
       "      <th>n_bonds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "      <td>-0.012698</td>\n",
       "      <td>1.085804</td>\n",
       "      <td>0.008001</td>\n",
       "      <td>2.55</td>\n",
       "      <td>0.82</td>\n",
       "      <td>1.091950</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>1</td>\n",
       "      <td>H</td>\n",
       "      <td>0.002150</td>\n",
       "      <td>-0.006031</td>\n",
       "      <td>0.001976</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.091953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>2</td>\n",
       "      <td>H</td>\n",
       "      <td>1.011731</td>\n",
       "      <td>1.463751</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.091952</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>3</td>\n",
       "      <td>H</td>\n",
       "      <td>-0.540815</td>\n",
       "      <td>1.447527</td>\n",
       "      <td>-0.876644</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.091946</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>4</td>\n",
       "      <td>H</td>\n",
       "      <td>-0.523814</td>\n",
       "      <td>1.437933</td>\n",
       "      <td>0.906397</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.091948</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dsgdb9nsd_000002</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>-0.040426</td>\n",
       "      <td>1.024108</td>\n",
       "      <td>0.062564</td>\n",
       "      <td>3.04</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.017195</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dsgdb9nsd_000002</td>\n",
       "      <td>1</td>\n",
       "      <td>H</td>\n",
       "      <td>0.017257</td>\n",
       "      <td>0.012545</td>\n",
       "      <td>-0.027377</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.017190</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dsgdb9nsd_000002</td>\n",
       "      <td>2</td>\n",
       "      <td>H</td>\n",
       "      <td>0.915789</td>\n",
       "      <td>1.358745</td>\n",
       "      <td>-0.028758</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.017187</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dsgdb9nsd_000002</td>\n",
       "      <td>3</td>\n",
       "      <td>H</td>\n",
       "      <td>-0.520278</td>\n",
       "      <td>1.343532</td>\n",
       "      <td>-0.775543</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.017208</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>dsgdb9nsd_000003</td>\n",
       "      <td>0</td>\n",
       "      <td>O</td>\n",
       "      <td>-0.034360</td>\n",
       "      <td>0.977540</td>\n",
       "      <td>0.007602</td>\n",
       "      <td>3.44</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.962107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>dsgdb9nsd_000003</td>\n",
       "      <td>1</td>\n",
       "      <td>H</td>\n",
       "      <td>0.064766</td>\n",
       "      <td>0.020572</td>\n",
       "      <td>0.001535</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.962107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>dsgdb9nsd_000003</td>\n",
       "      <td>2</td>\n",
       "      <td>H</td>\n",
       "      <td>0.871790</td>\n",
       "      <td>1.300792</td>\n",
       "      <td>0.000693</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.962107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>dsgdb9nsd_000004</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "      <td>0.599539</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.55</td>\n",
       "      <td>0.82</td>\n",
       "      <td>1.130589</td>\n",
       "      <td>0.068490</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>dsgdb9nsd_000004</td>\n",
       "      <td>1</td>\n",
       "      <td>C</td>\n",
       "      <td>-0.599539</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.55</td>\n",
       "      <td>0.82</td>\n",
       "      <td>1.130589</td>\n",
       "      <td>0.068490</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>dsgdb9nsd_000004</td>\n",
       "      <td>2</td>\n",
       "      <td>H</td>\n",
       "      <td>-1.661639</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.062099</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>dsgdb9nsd_000004</td>\n",
       "      <td>3</td>\n",
       "      <td>H</td>\n",
       "      <td>1.661639</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.062099</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>dsgdb9nsd_000005</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "      <td>-0.013324</td>\n",
       "      <td>1.132466</td>\n",
       "      <td>0.008276</td>\n",
       "      <td>2.55</td>\n",
       "      <td>0.82</td>\n",
       "      <td>1.109173</td>\n",
       "      <td>0.042575</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>dsgdb9nsd_000005</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>0.002311</td>\n",
       "      <td>-0.019159</td>\n",
       "      <td>0.001929</td>\n",
       "      <td>3.04</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.151748</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>dsgdb9nsd_000005</td>\n",
       "      <td>2</td>\n",
       "      <td>H</td>\n",
       "      <td>-0.027803</td>\n",
       "      <td>2.198949</td>\n",
       "      <td>0.014154</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.066598</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>dsgdb9nsd_000007</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "      <td>-0.018704</td>\n",
       "      <td>1.525582</td>\n",
       "      <td>0.010433</td>\n",
       "      <td>2.55</td>\n",
       "      <td>0.82</td>\n",
       "      <td>1.203627</td>\n",
       "      <td>0.188217</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       molecule_name  atom_index atom         x         y         z    EN  \\\n",
       "0   dsgdb9nsd_000001           0    C -0.012698  1.085804  0.008001  2.55   \n",
       "1   dsgdb9nsd_000001           1    H  0.002150 -0.006031  0.001976  2.20   \n",
       "2   dsgdb9nsd_000001           2    H  1.011731  1.463751  0.000277  2.20   \n",
       "3   dsgdb9nsd_000001           3    H -0.540815  1.447527 -0.876644  2.20   \n",
       "4   dsgdb9nsd_000001           4    H -0.523814  1.437933  0.906397  2.20   \n",
       "5   dsgdb9nsd_000002           0    N -0.040426  1.024108  0.062564  3.04   \n",
       "6   dsgdb9nsd_000002           1    H  0.017257  0.012545 -0.027377  2.20   \n",
       "7   dsgdb9nsd_000002           2    H  0.915789  1.358745 -0.028758  2.20   \n",
       "8   dsgdb9nsd_000002           3    H -0.520278  1.343532 -0.775543  2.20   \n",
       "9   dsgdb9nsd_000003           0    O -0.034360  0.977540  0.007602  3.44   \n",
       "10  dsgdb9nsd_000003           1    H  0.064766  0.020572  0.001535  2.20   \n",
       "11  dsgdb9nsd_000003           2    H  0.871790  1.300792  0.000693  2.20   \n",
       "12  dsgdb9nsd_000004           0    C  0.599539  0.000000  1.000000  2.55   \n",
       "13  dsgdb9nsd_000004           1    C -0.599539  0.000000  1.000000  2.55   \n",
       "14  dsgdb9nsd_000004           2    H -1.661639  0.000000  1.000000  2.20   \n",
       "15  dsgdb9nsd_000004           3    H  1.661639  0.000000  1.000000  2.20   \n",
       "16  dsgdb9nsd_000005           0    C -0.013324  1.132466  0.008276  2.55   \n",
       "17  dsgdb9nsd_000005           1    N  0.002311 -0.019159  0.001929  3.04   \n",
       "18  dsgdb9nsd_000005           2    H -0.027803  2.198949  0.014154  2.20   \n",
       "19  dsgdb9nsd_000007           0    C -0.018704  1.525582  0.010433  2.55   \n",
       "\n",
       "     rad  bond_lengths_mean  bond_lengths_std  n_bonds  \n",
       "0   0.82           1.091950          0.000003        4  \n",
       "1   0.43           1.091953          0.000000        1  \n",
       "2   0.43           1.091952          0.000000        1  \n",
       "3   0.43           1.091946          0.000000        1  \n",
       "4   0.43           1.091948          0.000000        1  \n",
       "5   0.80           1.017195          0.000009        3  \n",
       "6   0.43           1.017190          0.000000        1  \n",
       "7   0.43           1.017187          0.000000        1  \n",
       "8   0.43           1.017208          0.000000        1  \n",
       "9   0.78           0.962107          0.000000        2  \n",
       "10  0.43           0.962107          0.000000        1  \n",
       "11  0.43           0.962107          0.000000        1  \n",
       "12  0.82           1.130589          0.068490        2  \n",
       "13  0.82           1.130589          0.068490        2  \n",
       "14  0.43           1.062099          0.000000        1  \n",
       "15  0.43           1.062099          0.000000        1  \n",
       "16  0.82           1.109173          0.042575        2  \n",
       "17  0.80           1.151748          0.000000        1  \n",
       "18  0.43           1.066598          0.000000        1  \n",
       "19  0.82           1.203627          0.188217        4  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "i_atom = structures['atom_index'].values\n",
    "p = structures[['x', 'y', 'z']].values\n",
    "p_compare = p\n",
    "m = structures['molecule_name'].values\n",
    "m_compare = m\n",
    "r = structures['rad'].values\n",
    "r_compare = r\n",
    "\n",
    "source_row = np.arange(len(structures))\n",
    "max_atoms = 28\n",
    "\n",
    "bonds = np.zeros((len(structures)+1, max_atoms+1), dtype=np.int8)\n",
    "bond_dists = np.zeros((len(structures)+1, max_atoms+1), dtype=np.float32)\n",
    "\n",
    "print('Calculating bonds')\n",
    "\n",
    "for i in tqdm(range(max_atoms-1)):\n",
    "    p_compare = np.roll(p_compare, -1, axis=0)\n",
    "    m_compare = np.roll(m_compare, -1, axis=0)\n",
    "    r_compare = np.roll(r_compare, -1, axis=0)\n",
    "    \n",
    "    mask = np.where(m == m_compare, 1, 0) #Are we still comparing atoms in the same molecule?\n",
    "    dists = np.linalg.norm(p - p_compare, axis=1) * mask\n",
    "    r_bond = r + r_compare\n",
    "    \n",
    "    bond = np.where(np.logical_and(dists > 0.0001, dists < r_bond), 1, 0)\n",
    "    \n",
    "    source_row = source_row\n",
    "    target_row = source_row + i + 1 #Note: Will be out of bounds of bonds array for some values of i\n",
    "    target_row = np.where(np.logical_or(target_row > len(structures), mask==0), len(structures), target_row) #If invalid target, write to dummy row\n",
    "    \n",
    "    source_atom = i_atom\n",
    "    target_atom = i_atom + i + 1 #Note: Will be out of bounds of bonds array for some values of i\n",
    "    target_atom = np.where(np.logical_or(target_atom > max_atoms, mask==0), max_atoms, target_atom) #If invalid target, write to dummy col\n",
    "    \n",
    "    bonds[(source_row, target_atom)] = bond\n",
    "    bonds[(target_row, source_atom)] = bond\n",
    "    bond_dists[(source_row, target_atom)] = dists\n",
    "    bond_dists[(target_row, source_atom)] = dists\n",
    "\n",
    "bonds = np.delete(bonds, axis=0, obj=-1) #Delete dummy row\n",
    "bonds = np.delete(bonds, axis=1, obj=-1) #Delete dummy col\n",
    "bond_dists = np.delete(bond_dists, axis=0, obj=-1) #Delete dummy row\n",
    "bond_dists = np.delete(bond_dists, axis=1, obj=-1) #Delete dummy col\n",
    "\n",
    "print('Counting and condensing bonds')\n",
    "\n",
    "bonds_numeric = [[i for i,x in enumerate(row) if x] for row in tqdm(bonds)]\n",
    "bond_lengths = [[dist for i,dist in enumerate(row) if i in bonds_numeric[j]] for j,row in enumerate(tqdm(bond_dists))]\n",
    "bond_lengths_mean = [ np.mean(x) for x in bond_lengths]\n",
    "bond_lengths_std = [ np.std(x) for x in bond_lengths]\n",
    "n_bonds = [len(x) for x in bonds_numeric]\n",
    "\n",
    "#bond_data = {'bond_' + str(i):col for i, col in enumerate(np.transpose(bonds))}\n",
    "#bond_data.update({'bonds_numeric':bonds_numeric, 'n_bonds':n_bonds})\n",
    "\n",
    "bond_data = {'n_bonds':n_bonds, 'bond_lengths_mean': bond_lengths_mean,'bond_lengths_std':bond_lengths_std }\n",
    "bond_df = pd.DataFrame(bond_data)\n",
    "structures = structures.join(bond_df)\n",
    "display(structures.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_atom_info(df, atom_idx):\n",
    "    df = pd.merge(df, structures, how = 'left',\n",
    "                  left_on  = ['molecule_name', f'atom_index_{atom_idx}'],\n",
    "                  right_on = ['molecule_name',  'atom_index'])\n",
    "    \n",
    "    df = df.drop('atom_index', axis=1)\n",
    "    df = df.rename(columns={'atom': f'atom_{atom_idx}',\n",
    "                            'x': f'x_{atom_idx}',\n",
    "                            'y': f'y_{atom_idx}',\n",
    "                            'z': f'z_{atom_idx}'})\n",
    "    df = reduce_mem_usage(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 244.33 Mb (57.0% reduction)\n",
      "Mem. usage decreased to 346.50 Mb (38.6% reduction)\n",
      "Mem. usage decreased to 126.64 Mb (55.8% reduction)\n",
      "Mem. usage decreased to 181.60 Mb (39.2% reduction)\n"
     ]
    }
   ],
   "source": [
    "train = map_atom_info(train, 0)\n",
    "train = map_atom_info(train, 1)\n",
    "\n",
    "test = map_atom_info(test, 0)\n",
    "test = map_atom_info(test, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_p_0 = train[['x_0', 'y_0', 'z_0']].values\n",
    "train_p_1 = train[['x_1', 'y_1', 'z_1']].values\n",
    "test_p_0 = test[['x_0', 'y_0', 'z_0']].values\n",
    "test_p_1 = test[['x_1', 'y_1', 'z_1']].values\n",
    "\n",
    "train['dist'] = np.linalg.norm(train_p_0 - train_p_1, axis=1)\n",
    "test['dist'] = np.linalg.norm(test_p_0 - test_p_1, axis=1)\n",
    "train['dist_x'] = (train['x_0'] - train['x_1']) ** 2\n",
    "test['dist_x'] = (test['x_0'] - test['x_1']) ** 2\n",
    "train['dist_y'] = (train['y_0'] - train['y_1']) ** 2\n",
    "test['dist_y'] = (test['y_0'] - test['y_1']) ** 2\n",
    "train['dist_z'] = (train['z_0'] - train['z_1']) ** 2\n",
    "test['dist_z'] = (test['z_0'] - test['z_1']) ** 2\n",
    "\n",
    "'''\n",
    "This will create 2 features:\n",
    "\n",
    "    1) will show the first letter of `type`\n",
    "    2) Will show the rest of characters\n",
    "'''\n",
    "\n",
    "train['type_0'] = train['type'].apply(lambda x: x[0])\n",
    "test['type_0'] = test['type'].apply(lambda x: x[0])\n",
    "train['type_1'] = train['type'].apply(lambda x: x[1:])\n",
    "test['type_1'] = test['type'].apply(lambda x: x[1:])\n",
    "\n",
    "train['dist_to_type_mean'] = train['dist'] / train.groupby('type')['dist'].transform('mean')\n",
    "test['dist_to_type_mean'] = test['dist'] / test.groupby('type')['dist'].transform('mean')\n",
    "\n",
    "train['dist_to_type_0_mean'] = train['dist'] / train.groupby('type_0')['dist'].transform('mean')\n",
    "test['dist_to_type_0_mean'] = test['dist'] / test.groupby('type_0')['dist'].transform('mean')\n",
    "\n",
    "train['dist_to_type_1_mean'] = train['dist'] / train.groupby('type_1')['dist'].transform('mean')\n",
    "test['dist_to_type_1_mean'] = test['dist'] / test.groupby('type_1')['dist'].transform('mean')\n",
    "\n",
    "\n",
    "train[f'molecule_type_dist_mean'] = train.groupby(['molecule_name', 'type'])['dist'].transform('mean')\n",
    "test[f'molecule_type_dist_mean'] = test.groupby(['molecule_name', 'type'])['dist'].transform('mean')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df):\n",
    "    df['molecule_couples'] = df.groupby('molecule_name')['id'].transform('count')\n",
    "    df['molecule_dist_mean'] = df.groupby('molecule_name')['dist'].transform('mean')\n",
    "    df['molecule_dist_min'] = df.groupby('molecule_name')['dist'].transform('min')\n",
    "    df['molecule_dist_max'] = df.groupby('molecule_name')['dist'].transform('max')\n",
    "    df['atom_0_couples_count'] = df.groupby(['molecule_name', 'atom_index_0'])['id'].transform('count')\n",
    "    df['atom_1_couples_count'] = df.groupby(['molecule_name', 'atom_index_1'])['id'].transform('count')\n",
    "    \n",
    "    df[f'molecule_atom_index_0_x_1_std'] = df.groupby(['molecule_name', 'atom_index_0'])['x_1'].transform('std')\n",
    "    df[f'molecule_atom_index_0_y_1_mean'] = df.groupby(['molecule_name', 'atom_index_0'])['y_1'].transform('mean')\n",
    "    df[f'molecule_atom_index_0_y_1_mean_diff'] = df[f'molecule_atom_index_0_y_1_mean'] - df['y_1']\n",
    "    df[f'molecule_atom_index_0_y_1_mean_div'] = df[f'molecule_atom_index_0_y_1_mean'] / df['y_1']\n",
    "    df[f'molecule_atom_index_0_y_1_max'] = df.groupby(['molecule_name', 'atom_index_0'])['y_1'].transform('max')\n",
    "    df[f'molecule_atom_index_0_y_1_max_diff'] = df[f'molecule_atom_index_0_y_1_max'] - df['y_1']\n",
    "    df[f'molecule_atom_index_0_y_1_std'] = df.groupby(['molecule_name', 'atom_index_0'])['y_1'].transform('std')\n",
    "    df[f'molecule_atom_index_0_z_1_std'] = df.groupby(['molecule_name', 'atom_index_0'])['z_1'].transform('std')\n",
    "    df[f'molecule_atom_index_0_dist_mean'] = df.groupby(['molecule_name', 'atom_index_0'])['dist'].transform('mean')\n",
    "    df[f'molecule_atom_index_0_dist_mean_diff'] = df[f'molecule_atom_index_0_dist_mean'] - df['dist']\n",
    "    df[f'molecule_atom_index_0_dist_mean_div'] = df[f'molecule_atom_index_0_dist_mean'] / df['dist']\n",
    "    df[f'molecule_atom_index_0_dist_max'] = df.groupby(['molecule_name', 'atom_index_0'])['dist'].transform('max')\n",
    "    df[f'molecule_atom_index_0_dist_max_diff'] = df[f'molecule_atom_index_0_dist_max'] - df['dist']\n",
    "    df[f'molecule_atom_index_0_dist_max_div'] = df[f'molecule_atom_index_0_dist_max'] / df['dist']\n",
    "    df[f'molecule_atom_index_0_dist_min'] = df.groupby(['molecule_name', 'atom_index_0'])['dist'].transform('min')\n",
    "    df[f'molecule_atom_index_0_dist_min_diff'] = df[f'molecule_atom_index_0_dist_min'] - df['dist']\n",
    "    df[f'molecule_atom_index_0_dist_min_div'] = df[f'molecule_atom_index_0_dist_min'] / df['dist']\n",
    "    df[f'molecule_atom_index_0_dist_std'] = df.groupby(['molecule_name', 'atom_index_0'])['dist'].transform('std')\n",
    "    df[f'molecule_atom_index_0_dist_std_diff'] = df[f'molecule_atom_index_0_dist_std'] - df['dist']\n",
    "    df[f'molecule_atom_index_0_dist_std_div'] = df[f'molecule_atom_index_0_dist_std'] / df['dist']\n",
    "    df[f'molecule_atom_index_1_dist_mean'] = df.groupby(['molecule_name', 'atom_index_1'])['dist'].transform('mean')\n",
    "    df[f'molecule_atom_index_1_dist_mean_diff'] = df[f'molecule_atom_index_1_dist_mean'] - df['dist']\n",
    "    df[f'molecule_atom_index_1_dist_mean_div'] = df[f'molecule_atom_index_1_dist_mean'] / df['dist']\n",
    "    df[f'molecule_atom_index_1_dist_max'] = df.groupby(['molecule_name', 'atom_index_1'])['dist'].transform('max')\n",
    "    df[f'molecule_atom_index_1_dist_max_diff'] = df[f'molecule_atom_index_1_dist_max'] - df['dist']\n",
    "    df[f'molecule_atom_index_1_dist_max_div'] = df[f'molecule_atom_index_1_dist_max'] / df['dist']\n",
    "    df[f'molecule_atom_index_1_dist_min'] = df.groupby(['molecule_name', 'atom_index_1'])['dist'].transform('min')\n",
    "    df[f'molecule_atom_index_1_dist_min_diff'] = df[f'molecule_atom_index_1_dist_min'] - df['dist']\n",
    "    df[f'molecule_atom_index_1_dist_min_div'] = df[f'molecule_atom_index_1_dist_min'] / df['dist']\n",
    "    df[f'molecule_atom_index_1_dist_std'] = df.groupby(['molecule_name', 'atom_index_1'])['dist'].transform('std')\n",
    "    df[f'molecule_atom_index_1_dist_std_diff'] = df[f'molecule_atom_index_1_dist_std'] - df['dist']\n",
    "    df[f'molecule_atom_index_1_dist_std_div'] = df[f'molecule_atom_index_1_dist_std'] / df['dist']\n",
    "    df[f'molecule_atom_1_dist_mean'] = df.groupby(['molecule_name', 'atom_1'])['dist'].transform('mean')\n",
    "    df[f'molecule_atom_1_dist_min'] = df.groupby(['molecule_name', 'atom_1'])['dist'].transform('min')\n",
    "    df[f'molecule_atom_1_dist_min_diff'] = df[f'molecule_atom_1_dist_min'] - df['dist']\n",
    "    df[f'molecule_atom_1_dist_min_div'] = df[f'molecule_atom_1_dist_min'] / df['dist']\n",
    "    df[f'molecule_atom_1_dist_std'] = df.groupby(['molecule_name', 'atom_1'])['dist'].transform('std')\n",
    "    df[f'molecule_atom_1_dist_std_diff'] = df[f'molecule_atom_1_dist_std'] - df['dist']\n",
    "    df[f'molecule_type_0_dist_std'] = df.groupby(['molecule_name', 'type_0'])['dist'].transform('std')\n",
    "    df[f'molecule_type_0_dist_std_diff'] = df[f'molecule_type_0_dist_std'] - df['dist']\n",
    "    df[f'molecule_type_dist_mean'] = df.groupby(['molecule_name', 'type'])['dist'].transform('mean')\n",
    "    df[f'molecule_type_dist_mean_diff'] = df[f'molecule_type_dist_mean'] - df['dist']\n",
    "    df[f'molecule_type_dist_mean_div'] = df[f'molecule_type_dist_mean'] / df['dist']\n",
    "    df[f'molecule_type_dist_max'] = df.groupby(['molecule_name', 'type'])['dist'].transform('max')\n",
    "    df[f'molecule_type_dist_min'] = df.groupby(['molecule_name', 'type'])['dist'].transform('min')\n",
    "    df[f'molecule_type_dist_std'] = df.groupby(['molecule_name', 'type'])['dist'].transform('std')\n",
    "    df[f'molecule_type_dist_std_diff'] = df[f'molecule_type_dist_std'] - df['dist']\n",
    "\n",
    "    df = reduce_mem_usage(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_atom_info_2df(df_1,df_2, atom_idx):\n",
    "    df = pd.merge(df_1, df_2, how = 'left',\n",
    "                  left_on  = ['molecule_name', f'atom_index_{atom_idx}'],\n",
    "                  right_on = ['molecule_name',  'atom_index'])\n",
    "    df = df.drop('atom_index', axis=1)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_closest(df):\n",
    "    df_temp = df.loc[:,[\"molecule_name\",\"atom_index_0\",\"atom_index_1\",\"dist\",\"x_0\",\"y_0\",\"z_0\",\"x_1\",\"y_1\",\"z_1\"]].copy()\n",
    "    df_temp_ = df_temp.copy()\n",
    "    df_temp_ = df_temp_.rename(columns={'atom_index_0': 'atom_index_1',\n",
    "                                       'atom_index_1': 'atom_index_0',\n",
    "                                       'x_0': 'x_1',\n",
    "                                       'y_0': 'y_1',\n",
    "                                       'z_0': 'z_1',\n",
    "                                       'x_1': 'x_0',\n",
    "                                       'y_1': 'y_0',\n",
    "                                       'z_1': 'z_0'})\n",
    "    df_temp = pd.concat(objs=[df_temp,df_temp_],axis=0)\n",
    "\n",
    "    df_temp[\"min_distance\"] = df_temp.groupby(['molecule_name', 'atom_index_0'])['dist'].transform('min')\n",
    "    df_temp = df_temp[df_temp[\"min_distance\"]==df_temp[\"dist\"]]\n",
    "\n",
    "    df_temp = df_temp.drop(['x_0','y_0','z_0','min_distance'], axis=1)\n",
    "    df_temp = df_temp.rename(columns={'atom_index_0': 'atom_index',\n",
    "                                     'atom_index_1': 'atom_index_closest',\n",
    "                                     'distance': 'distance_closest',\n",
    "                                     'x_1': 'x_closest',\n",
    "                                     'y_1': 'y_closest',\n",
    "                                     'z_1': 'z_closest'})\n",
    "\n",
    "    for atom_idx in [0,1]:\n",
    "        df = map_atom_info_2df(df,df_temp, atom_idx)\n",
    "        df = df.rename(columns={'atom_index_closest': f'atom_index_closest_{atom_idx}',\n",
    "                                            'distance_closest': f'distance_closest_{atom_idx}',\n",
    "                                            'x_closest': f'x_closest_{atom_idx}',\n",
    "                                            'y_closest': f'y_closest_{atom_idx}',\n",
    "                                            'z_closest': f'z_closest_{atom_idx}'})  \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cosine Angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_features(df):\n",
    "    df[\"distance_0\"]=((df['x_0']-df['x_closest_0'])**2+(df['y_0']-df['y_closest_0'])**2+(df['z_0']-df['z_closest_0'])**2)**(1/2)\n",
    "    df[\"distance_1\"]=((df['x_1']-df['x_closest_1'])**2+(df['y_1']-df['y_closest_1'])**2+(df['z_1']-df['z_closest_1'])**2)**(1/2)\n",
    "    df[\"vec_0_x\"]=(df['x_0']-df['x_closest_0'])/df[\"distance_0\"]\n",
    "    df[\"vec_0_y\"]=(df['y_0']-df['y_closest_0'])/df[\"distance_0\"]\n",
    "    df[\"vec_0_z\"]=(df['z_0']-df['z_closest_0'])/df[\"distance_0\"]\n",
    "    df[\"vec_1_x\"]=(df['x_1']-df['x_closest_1'])/df[\"distance_1\"]\n",
    "    df[\"vec_1_y\"]=(df['y_1']-df['y_closest_1'])/df[\"distance_1\"]\n",
    "    df[\"vec_1_z\"]=(df['z_1']-df['z_closest_1'])/df[\"distance_1\"]\n",
    "    df[\"vec_x\"]=(df['x_1']-df['x_0'])/df[\"dist\"]\n",
    "    df[\"vec_y\"]=(df['y_1']-df['y_0'])/df[\"dist\"]\n",
    "    df[\"vec_z\"]=(df['z_1']-df['z_0'])/df[\"dist\"]\n",
    "    df[\"cos_0_1\"]=df[\"vec_0_x\"]*df[\"vec_1_x\"]+df[\"vec_0_y\"]*df[\"vec_1_y\"]+df[\"vec_0_z\"]*df[\"vec_1_z\"]\n",
    "    df[\"cos_0\"]=df[\"vec_0_x\"]*df[\"vec_x\"]+df[\"vec_0_y\"]*df[\"vec_y\"]+df[\"vec_0_z\"]*df[\"vec_z\"]\n",
    "    df[\"cos_1\"]=df[\"vec_1_x\"]*df[\"vec_x\"]+df[\"vec_1_y\"]*df[\"vec_y\"]+df[\"vec_1_z\"]*df[\"vec_z\"]\n",
    "    df=df.drop(['vec_0_x','vec_0_y','vec_0_z','vec_1_x','vec_1_y','vec_1_z','vec_x','vec_y','vec_z'], axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bonds Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT USED\n",
    "\n",
    "def bonds_compute(df):\n",
    "    atom_rad = [self.atomic_radius[x] for x in df['atom'].values]\n",
    "    df['rad'] = atom_rad\n",
    "    position = df[['x','y','z']].values\n",
    "    p_temp = position\n",
    "    molec_name = df['molecule_name'].values\n",
    "    m_temp = molec_name\n",
    "    radius = df['rad'].values\n",
    "    r_temp = radius\n",
    "    bond = 0\n",
    "    dist_keep = 0\n",
    "    dist_bond = 0 \n",
    "    no_bond = 0\n",
    "    dist_no_bond = 0\n",
    "    dist_matrix = np.zeros((df.shape[0],2*29))\n",
    "    dist_matrix_bond = np.zeros((df.shape[0],2*29))\n",
    "    dist_matrix_no_bond = np.zeros((df.shape[0],2*29))\n",
    "\n",
    "    for i in range(29):\n",
    "        p_temp = np.roll(p_temp,-1,axis=0)\n",
    "        m_temp = np.roll(m_temp,-1,axis=0)\n",
    "        r_temp = np.roll(r_temp,-1,axis=0)\n",
    "        mask = (m_temp==molec_name)\n",
    "        dist = np.linalg.norm(position-p_temp,axis=1) * mask            \n",
    "        dist_temp = np.roll(np.linalg.norm(position-p_temp,axis=1)*mask,i+1,axis=0)\n",
    "        diff_radius_dist = (dist-(radius+r_temp)) * (dist<(radius+r_temp)) * mask\n",
    "        diff_radius_dist_temp = np.roll(diff_radius_dist,i+1,axis=0)\n",
    "        bond += (dist<(radius+r_temp)) * mask\n",
    "        bond_temp = np.roll((dist<(radius+r_temp)) * mask,i+1,axis=0)\n",
    "        no_bond += (dist>=(radius+r_temp)) * mask\n",
    "        no_bond_temp = np.roll((dist>=(radius+r_temp)) * mask,i+1,axis=0)\n",
    "        bond += bond_temp\n",
    "        no_bond += no_bond_temp\n",
    "        dist_keep += dist * mask\n",
    "        dist_matrix[:,2*i] = dist\n",
    "        dist_matrix[:,2*i+1] = dist_temp\n",
    "        dist_matrix_bond[:,2*i] = dist * (dist<(radius+r_temp)) * mask\n",
    "        dist_matrix_bond[:,2*i+1] = dist_temp * bond_temp\n",
    "        dist_matrix_no_bond[:,2*i] = dist * (dist>(radius+r_temp)) * mask\n",
    "        dist_matrix_no_bond[:,2*i+1] = dist_temp * no_bond_temp\n",
    "    df['n_bonds'] = bond\n",
    "    df['n_no_bonds'] = no_bond\n",
    "    df['dist_mean'] = np.nanmean(np.where(dist_matrix==0,np.nan,dist_matrix), axis=1)\n",
    "    df['dist_median'] = np.nanmedian(np.where(dist_matrix==0,np.nan,dist_matrix), axis=1)\n",
    "    df['dist_std_bond'] = np.nanstd(np.where(dist_matrix_bond==0,np.nan,dist_matrix), axis=1)\n",
    "    df['dist_mean_bond'] = np.nanmean(np.where(dist_matrix_bond==0,np.nan,dist_matrix), axis=1)\n",
    "    df['dist_median_bond'] = np.nanmedian(np.where(dist_matrix_bond==0,np.nan,dist_matrix), axis=1)\n",
    "    df['dist_mean_no_bond'] = np.nanmean(np.where(dist_matrix_no_bond==0,np.nan,dist_matrix), axis=1)\n",
    "    df['dist_std_no_bond'] = np.nanstd(np.where(dist_matrix_no_bond==0,np.nan,dist_matrix), axis=1)\n",
    "    df['dist_median_no_bond'] = np.nanmedian(np.where(dist_matrix_no_bond==0,np.nan,dist_matrix), axis=1)\n",
    "    df['dist_std'] = np.nanstd(np.where(dist_matrix==0,np.nan,dist_matrix), axis=1)\n",
    "    df['dist_min'] = np.nanmin(np.where(dist_matrix==0,np.nan,dist_matrix), axis=1)\n",
    "    df['dist_max'] = np.nanmax(np.where(dist_matrix==0,np.nan,dist_matrix), axis=1)\n",
    "    df['range_dist'] = np.absolute(X['dist_max']-df['dist_min'])\n",
    "    df['dist_bond_min'] = np.nanmin(np.where(dist_matrix_bond==0,np.nan,dist_matrix), axis=1)\n",
    "    df['dist_bond_max'] = np.nanmax(np.where(dist_matrix_bond==0,np.nan,dist_matrix), axis=1)\n",
    "    df['range_dist_bond'] = np.absolute(X['dist_bond_max']-df['dist_bond_min'])\n",
    "    df['dist_no_bond_min'] = np.nanmin(np.where(dist_matrix_no_bond==0,np.nan,dist_matrix), axis=1)\n",
    "    df['dist_no_bond_max'] = np.nanmax(np.where(dist_matrix_no_bond==0,np.nan,dist_matrix), axis=1)\n",
    "    df['range_dist_no_bond'] = np.absolute(X['dist_no_bond_max']-df['dist_no_bond_min'])\n",
    "    df['n_diff'] = pd.DataFrame(np.around(dist_matrix_bond,5)).nunique(axis=1).values  #5\n",
    "    \n",
    "    df = reduce_mem_usage(X)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#train = bonds_compute(train)\n",
    "#test = bonds_compute(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 968.43 Mb (0.9% reduction)\n",
      "Mem. usage decreased to 516.13 Mb (0.9% reduction)\n"
     ]
    }
   ],
   "source": [
    "train = create_features(train)\n",
    "test = create_features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = create_closest(train)\n",
    "test = create_closest(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = cosine_features(train)\n",
    "test = cosine_features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_columns = ['type',\n",
    " 'bond_lengths_mean_y',\n",
    " 'bond_lengths_std_y',\n",
    " 'bond_lengths_mean_x',\n",
    " 'molecule_atom_index_0_dist_min_div',\n",
    " 'molecule_atom_index_0_dist_std_div',\n",
    " 'molecule_atom_index_0_dist_mean',\n",
    " 'molecule_atom_index_0_dist_max',\n",
    " 'dist_y',\n",
    " 'molecule_atom_index_1_dist_std_diff',\n",
    " 'z_0',\n",
    " 'molecule_type_dist_min',\n",
    " 'molecule_atom_index_0_y_1_mean_div',\n",
    " 'dist_x',\n",
    " 'x_0',\n",
    " 'y_0',\n",
    " 'molecule_type_dist_std',\n",
    " 'molecule_atom_index_0_y_1_std',\n",
    " 'molecule_dist_mean',\n",
    " 'molecule_atom_index_0_dist_std_diff',\n",
    " 'dist_z',\n",
    " 'molecule_atom_index_0_dist_std',\n",
    " 'molecule_atom_index_0_x_1_std',\n",
    " 'molecule_type_dist_std_diff',\n",
    " 'molecule_type_0_dist_std',\n",
    " 'dist',\n",
    " 'molecule_atom_index_0_dist_mean_diff',\n",
    " 'molecule_atom_index_1_dist_min_div',\n",
    " 'molecule_atom_index_1_dist_mean_diff',\n",
    " 'y_1',\n",
    " 'molecule_type_dist_mean_div',\n",
    " 'molecule_dist_max',\n",
    " 'molecule_atom_index_0_dist_mean_div',\n",
    " 'z_1',\n",
    " 'molecule_atom_index_0_z_1_std',\n",
    " 'molecule_atom_index_1_dist_mean_div',\n",
    " 'molecule_atom_index_1_dist_min_diff',\n",
    " 'molecule_atom_index_1_dist_mean',\n",
    " 'molecule_atom_index_1_dist_min',\n",
    " 'molecule_atom_index_1_dist_max',\n",
    " 'molecule_type_0_dist_std_diff',\n",
    " 'molecule_atom_index_0_dist_min_diff',\n",
    " 'molecule_type_dist_mean_diff',\n",
    " 'x_1',\n",
    " 'molecule_atom_index_0_y_1_max',\n",
    " 'molecule_atom_index_0_y_1_mean_diff',\n",
    " 'molecule_atom_1_dist_std_diff',\n",
    " 'molecule_atom_index_0_y_1_mean',\n",
    " 'molecule_atom_1_dist_std',\n",
    " 'molecule_type_dist_max']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals = train.select_dtypes(include='object').columns\n",
    "categoricals = test.select_dtypes(include='object').columns\n",
    "\n",
    "#Train Categoricals\n",
    "for c in categoricals:\n",
    "    lbl = LabelEncoder()\n",
    "    lbl.fit(list(train[c].values))\n",
    "    train[c] = lbl.transform(list(train[c].values))\n",
    "\n",
    "# Test Categoricals\n",
    "for c in categoricals:\n",
    "    lbl = LabelEncoder()\n",
    "    lbl.fit(list(test[c].values))\n",
    "    test[c] = lbl.transform(list(test[c].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the label\n",
    "y = train['scalar_coupling_constant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train[good_columns].copy()\n",
    "X_test = test[good_columns].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['dist_y', 'dist_x'], dtype='object')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicate_columns = X.columns[X.columns.duplicated()]\n",
    "duplicate_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop(['dist_y'], axis=1);\n",
    "X_test = X_test.drop(['dist_y'], axis=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop(['dist_x'], axis=1);\n",
    "X_test = X_test.drop(['dist_x'], axis=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([], dtype='object')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicate_columns = X.columns[X.columns.duplicated()]\n",
    "duplicate_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGB Matrix Creation\n",
    "dtrain = xgb.DMatrix(X, label=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting a 5-fold stratified cross-validation (note: shuffle=True)\n",
    "skf = KFold(n_splits=5, shuffle=True, random_state=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'booster' : 'gbtree',\n",
    "    # Parameters that we are going to tune.\n",
    "    'max_depth':8,\n",
    "    'min_child_weight': 1,\n",
    "    'eta':0.3,\n",
    "    'subsample': 1,\n",
    "    'colsample_bytree': 1,\n",
    "    # Other parameters\n",
    "    'objective':'reg:linear',\n",
    "    'eval_metric' : 'mae',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_dict_xgb = train_model_regression(X=X, X_test=X_test, y=y, params=params, folds=skf, model_type='xgb', eval_metric='group_mae', plot_feature_importance=True,\n",
    "                                        verbose=1000, early_stopping_rounds=16, n_estimators=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['scalar_coupling_constant'] = result_dict_xgb['prediction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('TRR_c_XGB_Molecular_Properties_3.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score of XXXX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot oof VS target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = pd.DataFrame(y)\n",
    "plot_data.index.name = 'id'\n",
    "plot_data['yhat'] = result_dict_xgb['oof']\n",
    "plot_data['type'] = lbl.inverse_transform(X['type'])\n",
    "\n",
    "def plot_oof_preds(ctype, llim, ulim):\n",
    "        plt.figure(figsize=(6,6))\n",
    "        sns.scatterplot(x='scalar_coupling_constant',y='yhat',\n",
    "                        data=plot_data.loc[plot_data['type']==ctype,\n",
    "                        ['scalar_coupling_constant', 'yhat']]);\n",
    "        plt.xlim((llim, ulim))\n",
    "        plt.ylim((llim, ulim))\n",
    "        plt.plot([llim, ulim], [llim, ulim])\n",
    "        plt.xlabel('scalar_coupling_constant')\n",
    "        plt.ylabel('predicted')\n",
    "        plt.title(f'{ctype}', fontsize=18)\n",
    "        plt.show()\n",
    "\n",
    "plot_oof_preds('1JHC', 0, 250)\n",
    "plot_oof_preds('1JHN', 0, 100)\n",
    "plot_oof_preds('2JHC', -50, 50)\n",
    "plot_oof_preds('2JHH', -50, 50)\n",
    "plot_oof_preds('2JHN', -25, 25)\n",
    "plot_oof_preds('3JHC', -25, 100)\n",
    "plot_oof_preds('3JHH', -20, 20)\n",
    "plot_oof_preds('3JHN', -15, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
